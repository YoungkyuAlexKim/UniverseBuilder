"""
AI ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ - Google Gemini AI í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬

ì´ ëª¨ë“ˆì€ ëª¨ë“  AI ê´€ë ¨ í˜¸ì¶œì„ ì¤‘ì•™í™”í•˜ì—¬ ê´€ë¦¬í•˜ë©°,
ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- ìŠ¤íŠ¸ë¦¬ë°/ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
- ìë™ ì¬ì‹œë„ ë¡œì§
- ì„¸ë¶€í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬
- JSON/í…ìŠ¤íŠ¸ ì‘ë‹µ ìë™ ì²˜ë¦¬
- ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
"""

import os
import sys
import json
import time
import logging
from typing import Union, Dict, Any, Optional, AsyncGenerator
from contextlib import asynccontextmanager

import google.generativeai as genai
from google.generativeai.types import GenerationConfig, HarmCategory, HarmBlockThreshold
from google.api_core import exceptions as google_exceptions
from fastapi import HTTPException
from dotenv import load_dotenv

from ..error_responses import create_error_response, ErrorCodes

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê¸°ë³¸ ì•ˆì „ ì„¤ì •
DEFAULT_SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# API í‚¤ ì„¤ì •ì€ call_ai_model í•¨ìˆ˜ ë‚´ì—ì„œ ë™ì ìœ¼ë¡œ ìˆ˜í–‰ë¨

# ë””ë²„ê¹…ìš©: ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©ëœ API í‚¤ ì •ë³´ ì €ì¥
last_used_api_key_info = None


async def call_ai_model(
    prompt: str,
    model_name: str,
    generation_config: Optional[GenerationConfig] = None,
    safety_settings: Optional[Dict[HarmCategory, HarmBlockThreshold]] = None,
    response_format: str = "auto",
    stream: bool = False,
    max_retries: int = 3,
    retry_delay: float = 1.0,
    user_api_key: Optional[str] = None,
) -> Union[Dict[str, Any], str, AsyncGenerator[str, None]]:
    """
    Google Gemini APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ì¤‘ì•™ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜.

    Args:
        prompt: AI ëª¨ë¸ì— ì „ë‹¬í•  ì „ì²´ í”„ë¡¬í”„íŠ¸.
        model_name: ì‚¬ìš©í•  AI ëª¨ë¸ ì´ë¦„.
        generation_config: ì‘ë‹µ í˜•ì‹ ë“±ì„ ì§€ì •í•˜ëŠ” ì„¤ì • ê°ì²´.
        safety_settings: ì•ˆì „ í•„í„° ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš© ì‹œ None).
        response_format: ì‘ë‹µ í˜•ì‹ ("json", "text", "auto").
        stream: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€.
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜.
        retry_delay: ì¬ì‹œë„ ê°„ê²© (ì´ˆ).
        user_api_key: ì‚¬ìš©ì ì œê³µ API í‚¤ (ì„ íƒì‚¬í•­).

    Returns:
        ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: AsyncGenerator[str, None]
        ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: dict (JSON) ë˜ëŠ” str (í…ìŠ¤íŠ¸)

    Raises:
        HTTPException: API í‚¤ ë¶€ì¬, ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨, ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ ë“± ì˜¤ë¥˜ ë°œìƒ ì‹œ.
    """
    start_time = time.time()

    # ì‚¬ìš©í•  API í‚¤ ê²°ì • (ì‚¬ìš©ì í‚¤ ìš°ì„ , ì—†ìœ¼ë©´ ì„œë²„ ê¸°ë³¸ í‚¤ë¡œ í´ë°±)
    default_api_key = os.getenv("GOOGLE_API_KEY")

    # PyInstaller í¬í„°ë¸” í™˜ê²½ì—ì„œëŠ” ì¶”ê°€ ê²½ë¡œì—ì„œ .env íŒŒì¼ ê²€ìƒ‰
    if not default_api_key and getattr(sys, 'frozen', False):
        try:
            # í¬í„°ë¸” í™˜ê²½ì—ì„œ .env íŒŒì¼ì´ ì‹¤í–‰ íŒŒì¼ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì„ ìˆ˜ ìˆìŒ
            import sys
            app_dir = os.path.dirname(sys.executable)
            env_path = os.path.join(app_dir, '.env')
            if os.path.exists(env_path):
                from dotenv import load_dotenv
                load_dotenv(env_path)
                default_api_key = os.getenv("GOOGLE_API_KEY")
                logger.info(f"í¬í„°ë¸” í™˜ê²½ì—ì„œ .env íŒŒì¼ ë¡œë“œ ì„±ê³µ: {env_path}")
        except Exception as e:
            logger.warning(f"í¬í„°ë¸” í™˜ê²½ .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ì‚¬ìš©ì í‚¤ ê²€ì¦ ë° ì •ë¦¬
    if user_api_key and isinstance(user_api_key, str):
        user_api_key = user_api_key.strip()

        # ì‚¬ìš©ì í‚¤ì˜ ê¸°ë³¸ì ì¸ ìœ íš¨ì„± ê²€ì¦
        if user_api_key and not user_api_key.startswith('AIza'):
            logger.warning(f"ì‚¬ìš©ì ì œê³µ API í‚¤ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {user_api_key[:10]}...")
            # í˜•ì‹ì´ ì˜ëª»ëœ í‚¤ëŠ” ë¬´ì‹œí•˜ê³  ì„œë²„ ê¸°ë³¸ í‚¤ ì‚¬ìš©
            user_api_key = None

        # ì‚¬ìš©ì í‚¤ì˜ ê¸¸ì´ ê²€ì¦ (Google AI API í‚¤ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 39ì ì´ìƒ)
        if user_api_key and len(user_api_key) < 39:
            logger.warning(f"ì‚¬ìš©ì ì œê³µ API í‚¤ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(user_api_key)}ì")
            # ê¸¸ì´ê°€ ì˜ëª»ëœ í‚¤ëŠ” ë¬´ì‹œí•˜ê³  ì„œë²„ ê¸°ë³¸ í‚¤ ì‚¬ìš©
            user_api_key = None

    # ì‚¬ìš©í•  API í‚¤ ê²°ì •
    api_key_to_use = user_api_key or default_api_key

    # ì‚¬ìš©ì í‚¤ ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
    if user_api_key and api_key_to_use == user_api_key:
        logger.info("ì‚¬ìš©ì ì œê³µ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    elif default_api_key:
        logger.info("ì„œë²„ ê¸°ë³¸ API í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # API í‚¤ ê²€ì¦
    if not api_key_to_use:
        logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ AI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš©ì í‚¤ì™€ ì„œë²„ ê¸°ë³¸ í‚¤ ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        error_response = create_error_response(
            error_code=ErrorCodes.AI_API_KEY_MISSING,
            message="ì‚¬ìš© ê°€ëŠ¥í•œ AI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.",
            user_message="AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        )
        raise HTTPException(status_code=500, detail=error_response.model_dump())

    # ê²°ì •ëœ API í‚¤ë¡œ Gemini ì„¤ì •
    try:
        genai.configure(api_key=api_key_to_use)

        # ë””ë²„ê¹…ìš©: ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©ëœ API í‚¤ ì •ë³´ ë¡œê¹…
        key_type = "ì‚¬ìš©ì í‚¤" if (user_api_key and api_key_to_use == user_api_key) else "ì„œë²„ í‚¤"
        masked_key = api_key_to_use[:10] + "..." + api_key_to_use[-4:] if len(api_key_to_use) > 14 else api_key_to_use

        logger.info(f"ğŸ¯ [ë””ë²„ê·¸] AI í˜¸ì¶œ - ì‚¬ìš©ëœ í‚¤: {key_type} | í‚¤ ë¯¸ë¦¬ë³´ê¸°: {masked_key} | í‚¤ ê¸¸ì´: {len(api_key_to_use)}ì")
        logger.info(f"API í‚¤ ì„¤ì • ì™„ë£Œ - í‚¤ ê¸¸ì´: {len(api_key_to_use)}ì")

        # ì „ì—­ ë³€ìˆ˜ì— ë§ˆì§€ë§‰ ì‚¬ìš© í‚¤ ì •ë³´ ì €ì¥ (ë””ë²„ê¹…ìš©)
        global last_used_api_key_info
        last_used_api_key_info = {
            "key_type": key_type,
            "masked_key": masked_key,
            "full_length": len(api_key_to_use),
            "timestamp": time.time(),
            "model": model_name,
            "has_user_key": bool(user_api_key),
            "has_server_key": bool(default_api_key)
        }

    except Exception as e:
        logger.error(f"API í‚¤ ì„¤ì • ì‹¤íŒ¨: {e}")
        error_response = create_error_response(
            error_code=ErrorCodes.AI_API_KEY_MISSING,
            message="AI API í‚¤ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            user_message="API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
        raise HTTPException(status_code=400, detail=error_response.model_dump())

    # ì•ˆì „ ì„¤ì • ì ìš©
    final_safety_settings = safety_settings or DEFAULT_SAFETY_SETTINGS

    # ë¡œê¹…: í˜¸ì¶œ ì‹œì‘
    logger.info(f"AI í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: {model_name}, ìŠ¤íŠ¸ë¦¬ë°: {stream}, í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}")

    try:
        model = genai.GenerativeModel(model_name)

        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ì²˜ë¦¬
        if stream:
            return _handle_streaming_response(
                model=model,
                prompt=prompt,
                generation_config=generation_config,
                safety_settings=final_safety_settings,
                start_time=start_time
            )

        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: ì¬ì‹œë„ ë¡œì§ ì ìš©
        for attempt in range(max_retries):
            try:
                response = await model.generate_content_async(
                    prompt,
                    generation_config=generation_config,
                    safety_settings=final_safety_settings
                )

                # ì‘ë‹µ ê²€ì¦
                result = _process_response(response, response_format)

                # ë¡œê¹…: ì„±ê³µ
                duration = time.time() - start_time
                logger.info(f"AI í˜¸ì¶œ ì„±ê³µ - ëª¨ë¸: {model_name}, ì²˜ë¦¬ì‹œê°„: {duration:.2f}s")
                return result

            except (google_exceptions.InternalServerError, google_exceptions.ServiceUnavailable) as e:
                if attempt < max_retries - 1:
                    logger.warning(f"AI ì„œë¹„ìŠ¤ ì¼ì‹œì  ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                    await asyncio.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    logger.error(f"AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜ë¡œ ì¸í•œ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                    error_response = create_error_response(
                        error_code=ErrorCodes.AI_GENERATION_FAILED,
                        message="AI ì„œë¹„ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤.",
                        user_message="ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    )
                    raise HTTPException(status_code=503, detail=error_response.model_dump())

    except HTTPException:
        raise
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"AI í˜¸ì¶œ ì‹¤íŒ¨ - ëª¨ë¸: {model_name}, ì²˜ë¦¬ì‹œê°„: {duration:.2f}s, ì˜¤ë¥˜: {e}")
        error_response = create_error_response(
            error_code=ErrorCodes.AI_GENERATION_FAILED,
            message=f"AI ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}",
            user_message="AI í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
        raise HTTPException(status_code=500, detail=error_response.model_dump())


async def _handle_streaming_response(
    model: genai.GenerativeModel,
    prompt: str,
    generation_config: Optional[GenerationConfig],
    safety_settings: Dict[HarmCategory, HarmBlockThreshold],
    start_time: float
) -> AsyncGenerator[str, None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
    """
    try:
        response = await model.generate_content_async(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings,
            stream=True
        )

        chunk_count = 0
        total_chars = 0

        async for chunk in response:
            if chunk.text:
                chunk_count += 1
                total_chars += len(chunk.text)
                yield f"data: {chunk.text}\n\n"

        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹ í˜¸
        yield "data: [DONE]\n\n"

        # ë¡œê¹…: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
        duration = time.time() - start_time
        logger.info(f"AI ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {duration:.2f}s")
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"AI ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ - ì²˜ë¦¬ì‹œê°„: {duration:.2f}s, ì˜¤ë¥˜: {e}")
        error_response = create_error_response(
            error_code=ErrorCodes.AI_GENERATION_FAILED,
            message=f"AI ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}",
            user_message="AI ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )
        yield f"data: {error_response.model_dump_json()}\n\n"


def _process_response(response: Any, response_format: str) -> Union[Dict[str, Any], str]:
    """
    AI ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜.
    """
    # ì‘ë‹µ ê²€ì¦
    if not response.parts:
        finish_reason = response.candidates[0].finish_reason.name if response.candidates else "UNKNOWN"
        if "SAFETY" in finish_reason:
            logger.warning(f"AI ì‘ë‹µì´ ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë¨: {finish_reason}")
            error_response = create_error_response(
                error_code=ErrorCodes.AI_GENERATION_FAILED,
                message="AI ìƒì„± ë‚´ìš©ì´ ì•ˆì „ í•„í„°ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.",
                user_message="ìš”ì²­í•˜ì‹  ë‚´ìš©ì´ AIì˜ ì•ˆì „ ì •ì±…ì— ìœ„ë°˜ë©ë‹ˆë‹¤."
            )
            raise HTTPException(status_code=400, detail=error_response.model_dump())

        logger.error(f"AIë¡œë¶€í„° ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í•¨: {finish_reason}")
        error_response = create_error_response(
            error_code=ErrorCodes.AI_GENERATION_FAILED,
            message=f"AIë¡œë¶€í„° ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            details={"finish_reason": finish_reason},
            user_message="AIê°€ ìœ íš¨í•œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
        raise HTTPException(status_code=500, detail=error_response.model_dump())

    # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    raw_text = response.text.strip()

    # ì‘ë‹µ í˜•ì‹ì— ë”°ë¥¸ ì²˜ë¦¬
    if response_format == "json" or (response_format == "auto" and _is_json_response(raw_text)):
        try:
            cleaned_text = raw_text.removeprefix("```json").removesuffix("```").strip()
            return json.loads(cleaned_text)
        except json.JSONDecodeError as e:
            logger.error(f"AI JSON ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}, ì›ë³¸: {raw_text[:200]}...")
            error_response = create_error_response(
                error_code=ErrorCodes.AI_RESPONSE_PARSING_FAILED,
                message="AIê°€ ìœ íš¨í•˜ì§€ ì•Šì€ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤.",
                details={"raw_response": raw_text[:200]},
                user_message="AI ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            )
            raise HTTPException(status_code=500, detail=error_response.model_dump())

    # í…ìŠ¤íŠ¸ ì‘ë‹µ
    return raw_text


def _is_json_response(text: str) -> bool:
    """
    ì‘ë‹µ í…ìŠ¤íŠ¸ê°€ JSON í˜•ì‹ì¸ì§€ íŒë‹¨í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    """
    cleaned = text.strip().removeprefix("```json").removesuffix("```").strip()
    try:
        json.loads(cleaned)
        return True
    except json.JSONDecodeError:
        return False


# ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ìœ„í•œ asyncio ì„í¬íŠ¸ (í•„ìš”í•œ ê²½ìš°)
import asyncio


def get_last_used_api_key_info():
    """
    ë””ë²„ê¹…ìš©: ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©ëœ API í‚¤ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    global last_used_api_key_info

    if last_used_api_key_info is None:
        return {
            "message": "ì•„ì§ AI í˜¸ì¶œì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "timestamp": None
        }

    # ì‹œê°„ ì •ë³´ ê°€ë…ì„± ìˆê²Œ ë³€í™˜
    import datetime
    readable_time = datetime.datetime.fromtimestamp(last_used_api_key_info["timestamp"]).strftime('%Y-%m-%d %H:%M:%S')

    return {
        **last_used_api_key_info,
        "readable_timestamp": readable_time,
        "time_since": f"{time.time() - last_used_api_key_info['timestamp']:.1f}ì´ˆ ì „"
    }
